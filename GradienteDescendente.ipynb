{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6FnlMiw9ZZpdRuZUSV/Rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raulc27/python_testes/blob/master/GradienteDescendente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "Vector = List[float]\n",
        "\n",
        "height_weight_age = [70,  # inches,\n",
        "                     170, # pounds,\n",
        "                     40 ] # years\n",
        "\n",
        "grades = [95,   # exam1\n",
        "          80,   # exam2\n",
        "          75,   # exam3\n",
        "          62 ]  # exam4\n",
        "\n",
        "def add(v: Vector, w: Vector) -> Vector:\n",
        "    \"\"\"Adds corresponding elements\"\"\"\n",
        "    assert len(v) == len(w), \"vectors must be the same length\"\n",
        "\n",
        "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
        "\n",
        "assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n",
        "\n",
        "def subtract(v: Vector, w: Vector) -> Vector:\n",
        "    \"\"\"Subtracts corresponding elements\"\"\"\n",
        "    assert len(v) == len(w), \"vectors must be the same length\"\n",
        "\n",
        "    return [v_i - w_i for v_i, w_i in zip(v, w)]\n",
        "\n",
        "assert subtract([5, 7, 9], [4, 5, 6]) == [1, 2, 3]\n",
        "\n",
        "def vector_sum(vectors: List[Vector]) -> Vector:\n",
        "    \"\"\"Sums all corresponding elements\"\"\"\n",
        "    # Check that vectors is not empty\n",
        "    assert vectors, \"no vectors provided!\"\n",
        "\n",
        "    # Check the vectors are all the same size\n",
        "    num_elements = len(vectors[0])\n",
        "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
        "\n",
        "    # the i-th element of the result is the sum of every vector[i]\n",
        "    return [sum(vector[i] for vector in vectors)\n",
        "            for i in range(num_elements)]\n",
        "\n",
        "assert vector_sum([[1, 2], [3, 4], [5, 6], [7, 8]]) == [16, 20]\n",
        "\n",
        "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
        "    \"\"\"Multiplies every element by c\"\"\"\n",
        "    return [c * v_i for v_i in v]\n",
        "\n",
        "assert scalar_multiply(2, [1, 2, 3]) == [2, 4, 6]\n",
        "\n",
        "def vector_mean(vectors: List[Vector]) -> Vector:\n",
        "    \"\"\"Computes the element-wise average\"\"\"\n",
        "    n = len(vectors)\n",
        "    return scalar_multiply(1/n, vector_sum(vectors))\n",
        "\n",
        "assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4]\n",
        "\n",
        "def dot(v: Vector, w: Vector) -> float:\n",
        "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
        "    assert len(v) == len(w), \"vectors must be same length\"\n",
        "\n",
        "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
        "\n",
        "assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6\n",
        "\n",
        "def sum_of_squares(v: Vector) -> float:\n",
        "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
        "    return dot(v, v)\n",
        "\n",
        "assert sum_of_squares([1, 2, 3]) == 14  # 1 * 1 + 2 * 2 + 3 * 3\n",
        "\n",
        "import math\n",
        "\n",
        "def magnitude(v: Vector) -> float:\n",
        "    \"\"\"Returns the magnitude (or length) of v\"\"\"\n",
        "    return math.sqrt(sum_of_squares(v))   # math.sqrt is square root function\n",
        "\n",
        "assert magnitude([3, 4]) == 5\n",
        "\n",
        "def squared_distance(v: Vector, w: Vector) -> float:\n",
        "    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
        "    return sum_of_squares(subtract(v, w))\n",
        "\n",
        "def distance(v: Vector, w: Vector) -> float:\n",
        "    \"\"\"Computes the distance between v and w\"\"\"\n",
        "    return math.sqrt(squared_distance(v, w))\n",
        "\n",
        "\n",
        "def distance(v: Vector, w: Vector) -> float:  # type: ignore\n",
        "    return magnitude(subtract(v, w))\n",
        "\n",
        "# Another type alias\n",
        "Matrix = List[List[float]]\n",
        "\n",
        "A = [[1, 2, 3],  # A has 2 rows and 3 columns\n",
        "     [4, 5, 6]]\n",
        "\n",
        "B = [[1, 2],     # B has 3 rows and 2 columns\n",
        "     [3, 4],\n",
        "     [5, 6]]\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "def shape(A: Matrix) -> Tuple[int, int]:\n",
        "    \"\"\"Returns (# of rows of A, # of columns of A)\"\"\"\n",
        "    num_rows = len(A)\n",
        "    num_cols = len(A[0]) if A else 0   # number of elements in first row\n",
        "    return num_rows, num_cols\n",
        "\n",
        "assert shape([[1, 2, 3], [4, 5, 6]]) == (2, 3)  # 2 rows, 3 columns\n",
        "\n",
        "def get_row(A: Matrix, i: int) -> Vector:\n",
        "    \"\"\"Returns the i-th row of A (as a Vector)\"\"\"\n",
        "    return A[i]             # A[i] is already the ith row\n",
        "\n",
        "def get_column(A: Matrix, j: int) -> Vector:\n",
        "    \"\"\"Returns the j-th column of A (as a Vector)\"\"\"\n",
        "    return [A_i[j]          # jth element of row A_i\n",
        "            for A_i in A]   # for each row A_i\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "def make_matrix(num_rows: int,\n",
        "                num_cols: int,\n",
        "                entry_fn: Callable[[int, int], float]) -> Matrix:\n",
        "    \"\"\"\n",
        "    Returns a num_rows x num_cols matrix\n",
        "    whose (i,j)-th entry is entry_fn(i, j)\n",
        "    \"\"\"\n",
        "    return [[entry_fn(i, j)             # given i, create a list\n",
        "             for j in range(num_cols)]  #   [entry_fn(i, 0), ... ]\n",
        "            for i in range(num_rows)]   # create one list for each i\n",
        "\n",
        "def identity_matrix(n: int) -> Matrix:\n",
        "    \"\"\"Returns the n x n identity matrix\"\"\"\n",
        "    return make_matrix(n, n, lambda i, j: 1 if i == j else 0)\n",
        "\n",
        "assert identity_matrix(5) == [[1, 0, 0, 0, 0],\n",
        "                              [0, 1, 0, 0, 0],\n",
        "                              [0, 0, 1, 0, 0],\n",
        "                              [0, 0, 0, 1, 0],\n",
        "                              [0, 0, 0, 0, 1]]\n",
        "\n",
        "data = [[70, 170, 40],\n",
        "        [65, 120, 26],\n",
        "        [77, 250, 19],\n",
        "        # ....\n",
        "       ]\n",
        "\n",
        "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
        "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
        "\n",
        "#            user 0  1  2  3  4  5  6  7  8  9\n",
        "#\n",
        "friend_matrix = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0],  # user 0\n",
        "                 [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],  # user 1\n",
        "                 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],  # user 2\n",
        "                 [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],  # user 3\n",
        "                 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],  # user 4\n",
        "                 [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],  # user 5\n",
        "                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 6\n",
        "                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 7\n",
        "                 [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],  # user 8\n",
        "                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]  # user 9\n",
        "\n",
        "assert friend_matrix[0][2] == 1, \"0 and 2 are friends\"\n",
        "assert friend_matrix[0][8] == 0, \"0 and 8 are not friends\"\n",
        "\n",
        "# only need to look at one row\n",
        "friends_of_five = [i\n",
        "                   for i, is_friend in enumerate(friend_matrix[5])\n",
        "                   if is_friend]"
      ],
      "metadata": {
        "id": "9cIlDJm-OLdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-K09UsFSXzs"
      },
      "outputs": [],
      "source": [
        "#from scratch import Vector, dot\n",
        "def sum_of_squares(v: Vector) -> float:\n",
        "  \"\"\"\n",
        "  Computa a soma de elementos quadrados em v\n",
        "  \"\"\"\n",
        "  return dot(v,v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "def difference_quotient(f: Callable[[float], float],\n",
        "                        x: float,\n",
        "                        h: float) -> float:\n",
        "    return (f(x+h)-f(x))/h"
      ],
      "metadata": {
        "id": "FSUixMBgPB6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square(x:float)->float:\n",
        "  return x*x\n",
        "\n",
        "def derivative(x: float) -> float:\n",
        "  return 2 * x"
      ],
      "metadata": {
        "id": "o4ZhwwszPVFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = range(-10,11)\n",
        "actuals = [derivative(x) for x in xs]\n",
        "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
        "\n",
        "# plote para indicar que eles são essencialmente os mesmos\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "plt.plot(xs, actuals, 'rx', label='Actual') # vermelho x\n",
        "plt.plot(xs, estimates, 'b+', label='Estimate') # azul +\n",
        "plt.legend(loc=9)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "VjeDEFkjMolv",
        "outputId": "e3cca017-02f3-4f0b-e9dd-7d0bf5b95b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                v: Vector,\n",
        "                                i: int,\n",
        "                                h: float) -> float:\n",
        "    \"\"\"\n",
        "    Retorna o quociente parcial das diferenças i de f em v\n",
        "    \"\"\"\n",
        "    w = [v_j + (h if j == i else 0) #adicione h somente ao elemento i de v\n",
        "         for j, v_j in enumerate(v)]\n",
        "    return (f(w) -f(v))/h"
      ],
      "metadata": {
        "id": "Ac74FAhuP_vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_gradient(f: Callable[[Vector], float],\n",
        "                      v: Vector,\n",
        "                      h: float=0.0001):\n",
        "  return [partial_difference_quotient(f, v, i, h)\n",
        "      for i in range(len(v))]"
      ],
      "metadata": {
        "id": "jbNp3obhQYUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o gradiente..."
      ],
      "metadata": {
        "id": "O7hfZL-Lc9sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "  \"\"\"\n",
        "  Move `step_size` na direção de `gradient` a partir de `v`\n",
        "  \"\"\"\n",
        "  assert len(v) == len(gradient)\n",
        "  step = scalar_multiply(step_size, gradient)\n",
        "  return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) ->  Vector:\n",
        "  return [2 * v_i for v_i in v]\n",
        "\n",
        "# selecione um ponto de partida aleatório\n",
        "v = [random.uniform(-10,10) for i in range(3)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  grad = sum_of_squares_gradient(v) # compute o gradiente em v\n",
        "  v = gradient_step(v, grad, -0.01) # dê um passo negativo para o gradiente\n",
        "  print(epoch, v)\n",
        "\n",
        "assert distance(v, [0,0,0]) < 0.001 # v deve ser o próximo de 0"
      ],
      "metadata": {
        "id": "xgdYWyRrc_-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x vai de -50 a 49, y é sempre 20 * x +5\n",
        "inputs = [(x, 20 * x +5) for x in range(-50,50)]"
      ],
      "metadata": {
        "id": "iP1oM0GdelUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
        "  slope, intercept = theta\n",
        "  predicted = slope * x + intercept # A previsão do modelo.\n",
        "  error = (predicted - y)           # o erro é (previsto - real)\n",
        "  squared_error = error ** 2        # Vamos minimizar o erro quadrático\n",
        "  grad = [2 * error * x, 2 * error] # usando seu gradiente\n",
        "  return grad"
      ],
      "metadata": {
        "id": "_bGrV7kHjqVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uso do vector_mean\n",
        "\n",
        "# comece com valores aleatórios para a inclinação e o intercepto\n",
        "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(5000):\n",
        "  # Compute a média dos gradientes\n",
        "  grad = vector_mean([linear_gradient(x,y,theta) for x, y in inputs])\n",
        "  # dê um passo nessa direção\n",
        "  theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "metadata": {
        "id": "_NGdbVnXlAtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minibatch e Gradiente Descendente Estocástico"
      ],
      "metadata": {
        "id": "ek0DCe6LuFwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypeVar, List, Iterator\n",
        "\n",
        "T = TypeVar('T') # isso permite a inserção de funções \"genéricas\"\n",
        "\n",
        "def minibatches(dataset: List[T],\n",
        "                batch_size: int,\n",
        "                shuffle: bool=True) -> Iterator[List[T]]:\n",
        "                \"\"\"\n",
        "                Gera minibatches de tamanho `btach_size` a partir do conjunto de dados\n",
        "                \"\"\"\n",
        "                # inicie os índices 0, batch_size, 2 * batch_size, ...\n",
        "                batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
        "\n",
        "                if shuffle: random.shuffle(batch_starts) #classifique os batches aleatoriamente\n",
        "\n",
        "                for start in batch_starts:\n",
        "                  end = start + batch_size\n",
        "                  yield dataset[start:end]"
      ],
      "metadata": {
        "id": "pyE3XjWkuI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
        "\n",
        "for epoch in range(1000):\n",
        "  for batch in minibatches(inputs, batch_size=20):\n",
        "    grad = vector_mean([linear_gradient(x,y,theta) for x, y in batch])\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "metadata": {
        "id": "Yg_s8rvivpum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradiente descendente estocástico\n",
        "\n",
        "theta = [random.uniform(-1,1), random.uniform(-1,1)]\n",
        "\n",
        "for epoch in range(100):\n",
        "  for x,y in inputs:\n",
        "    grad = linear_gradient(x, y, theta)\n",
        "    theta = gradient_step(theta, grad, -learning_rate)\n",
        "  print(epoch, theta)\n",
        "\n",
        "slope, intercept = theta\n",
        "assert 19.9 < slope < 20.1, \"slope should be about 20\"\n",
        "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
      ],
      "metadata": {
        "id": "tGbtHuTvwtGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5ju1Ii7wtFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}